{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca55da4",
   "metadata": {},
   "source": [
    "<h1 style = \"text-align: center; margin-top: 10px;\">WINTER SEMESTER 2022 - 23</h1>\n",
    "<h1 style = \"text-align: center; margin-top: 10px;\">MACHINE LEARNING</h1>\n",
    "<h1 style = \"text-align: center; margin-top: 10px;\">CSE4020</h1>\n",
    "<h1 style = \"text-align: center; margin-top: 10px;\">LAB ASSIGNMENT 3</h1>\n",
    "<br><br>\n",
    "<pre>\n",
    "    <b>Name:               </b> Gaurav Navada \n",
    "    <b>Registration no.:   </b> 20BKT0128 \n",
    "    <b>Date of submission: </b> 2/19/2023\n",
    "    <b>Slot no.:           </b> L43 + L44 \n",
    "    <b>Faculty Name:       </b> Prof. SUDHA.S\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a621a8f",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Write a program to implement k-Nearest Neighbour algorithm to classify the iris data set. Print both correct and wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4667c1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 2, Predicted class = 2\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 1, Predicted class = 1\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Correct prediction: Actual class = 0, Predicted class = 0\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a k-Nearest Neighbors classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the classes of the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Suppress the warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Print the correct and wrong predictions\n",
    "correct = 0\n",
    "wrong = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == y_pred[i]:\n",
    "        print(\"Correct prediction: Actual class = {}, Predicted class = {}\".format(y_test[i], y_pred[i]))\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(\"Wrong prediction: Actual class = {}, Predicted class = {}\".format(y_test[i], y_pred[i]))\n",
    "        wrong += 1\n",
    "\n",
    "# Print the accuracy of the classifier\n",
    "accuracy = correct / len(y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83e125",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Train SVM classifier using sklearn digits dataset( i.e from sklearn datasets import load_digits) and then: <br>\n",
    "a. Measure accuracy of your model using different kernels such as rbf and linear. <br>\n",
    "b. Tune your model further using regularization and gamma parameters and try tocome up highest accuracy score. <br>\n",
    "c. Use 80% of samples as training data size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c93b08",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "a. We then train an SVM classifier with two different kernels, 'linear' and 'rbf'. For each kernel, we create an SVC object and fit it to the training data using the fit method. We then use the trained classifier to predict the labels of the testing data using the predict method. We calculate the accuracy score of the classifier using the accuracy_score function from sklearn.metrics.<br>\n",
    "b. In this code, we define a parameter grid with different values of C and gamma. We then create an SVC object with 'rbf' kernel and use GridSearchCV to perform a grid search over the parameter grid. We fit the grid search object to the training data using the fit method. Note that we use the GridSearchCV class from sklearn to perform the grid search. This class performs cross-validation over the parameter grid and selects the best parameters based on the cross-validation score.<br>\n",
    "c. In this code, we first load the digits dataset using load_digits. We then split the dataset into training and testing sets using train_test_split. We use 80% of the samples as the training data size, as specified in the problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc9439f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score using linear kernel: 0.978\n",
      "Accuracy score using rbf kernel: 0.986\n",
      "Best parameters: {'C': 10, 'gamma': 0.001}\n",
      "Accuracy score: 0.990\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM classifier with different kernels\n",
    "kernels = ['linear', 'rbf']\n",
    "for kernel in kernels:\n",
    "    clf = SVC(kernel=kernel)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy score using {kernel} kernel: {acc:.3f}\")\n",
    "\n",
    "# Tune the SVM classifier using grid search\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "clf = SVC(kernel='rbf')\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy score: {grid_search.best_score_:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ba75a",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Build an Artificial Neural Network by implementing the Back propagation algorithm and test the same using appropriate data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fee2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.bias1 = np.zeros((1, self.hidden_size))\n",
    "        self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.bias2 = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        self.error = output - y\n",
    "        self.delta2 = np.multiply(self.error, self.sigmoid_derivative(self.z2))\n",
    "        self.d_weights2 = np.dot(self.a1.T, self.delta2)\n",
    "        self.d_bias2 = np.sum(self.delta2, axis=0)\n",
    "        \n",
    "        self.delta1 = np.dot(self.delta2, self.weights2.T) * self.sigmoid_derivative(self.z1)\n",
    "        self.d_weights1 = np.dot(X.T, self.delta1)\n",
    "        self.d_bias1 = np.sum(self.delta1, axis=0)\n",
    "        \n",
    "    def update_weights(self, learning_rate):\n",
    "        self.weights1 -= learning_rate * self.d_weights1\n",
    "        self.bias1 -= learning_rate * self.d_bias1\n",
    "        self.weights2 -= learning_rate * self.d_weights2\n",
    "        self.bias2 -= learning_rate * self.d_bias2\n",
    "        \n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for i in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "            self.update_weights(learning_rate)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5fd1ad",
   "metadata": {},
   "source": [
    "### Example of how to use this class to train a neural network on the XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45c796c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "[[0.04323574]\n",
      " [0.95885508]\n",
      " [0.95763163]\n",
      " [0.03653322]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork(2, 3, 1)\n",
    "nn.train(X, y, 0.1, 10000)\n",
    "\n",
    "print(\"Prediction:\")\n",
    "print(nn.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349abd0",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Bagging Ensembles including Bagged Decision Trees, Random Forest and Extra Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e10d1",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "In this example, we first load the Iris dataset and split it into training and testing sets. We then create three different bagging ensembles: a Bagged Decision Trees ensemble, a Random Forest ensemble, and an Extra Trees ensemble. We fit each ensemble to the training data and evaluate the accuracy of each ensemble on the test data. Finally, we print the accuracy score for each ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f8430ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Decision Trees accuracy: 0.9666666666666667\n",
      "Random Forest accuracy: 0.9666666666666667\n",
      "Extra Trees accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "# Create a Bagged Decision Trees ensemble\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)\n",
    "bagging.fit(X_train, y_train)\n",
    "print(\"Bagging Decision Trees accuracy:\", bagging.score(X_test, y_test))\n",
    "\n",
    "# Create a Random Forest ensemble\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Random Forest accuracy:\", rf.score(X_test, y_test))\n",
    "\n",
    "# Create an Extra Trees ensemble\n",
    "et = ExtraTreesClassifier(n_estimators=10)\n",
    "et.fit(X_train, y_train)\n",
    "print(\"Extra Trees accuracy:\", et.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564a71c1",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Boosting Ensembles including AdaBoost and Stochastic Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627e8ac",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "In this example, the Iris dataset is initially loaded and then divided into training and testing sets. Then, two distinct boosting ensembles—an AdaBoost ensemble and a Stochastic Gradient Boosting ensemble—are created. We analyse the accuracy of each ensemble using the test data after fitting each ensemble to the training set of data. The accuracy score for each ensemble is then printed.<br>We employ a decision tree with a maximum depth of one as the base estimator in the AdaBoost ensemble. This is so because AdaBoost performs best with simple and weak models, and a decision tree with a maximum depth of 1 is both. We employ decision trees with a maximum depth of 2 as the base estimators in the Gradient Boosting ensemble. This is due to the fact that Gradient Boosting functions best with complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40dcb40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost accuracy: 1.0\n",
      "Gradient Boosting accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "# Create an AdaBoost ensemble\n",
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=10)\n",
    "ada.fit(X_train, y_train)\n",
    "print(\"AdaBoost accuracy:\", ada.score(X_test, y_test))\n",
    "\n",
    "# Create a Gradient Boosting ensemble\n",
    "gb = GradientBoostingClassifier(max_depth=2, n_estimators=10)\n",
    "gb.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting accuracy:\", gb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec464a3e",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "Voting Ensembles for averaging the predictions for any arbitrary models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc41a3",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "In this example, we first load the Iris dataset and split it into training and testing sets. We then create three arbitrary models: a logistic regression model, a decision tree model, and a random forest model. We then create a voting ensemble of these three models using the 'VotingClassifier' class in scikit-learn. We set the 'voting' parameter to 'hard', which means the ensemble will make predictions by majority voting. We fit the ensemble to the training data and evaluate the accuracy of the ensemble on the test data. Finally, we print the accuracy score for the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "123b426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\n",
    "\n",
    "# Create some arbitrary models\n",
    "lr = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Create a voting ensemble of the arbitrary models\n",
    "ensemble = VotingClassifier(estimators=[('lr', lr), ('dt', dt), ('rf', rf)], voting='hard')\n",
    "ensemble.fit(X_train, y_train)\n",
    "print(\"Ensemble accuracy:\", ensemble.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
